{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyanarayana53/FMML_Projects_and_labs/blob/main/FMML_Module_2_project_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: Appreciating, Interpreting and Visualizing Data Project\n",
        "---"
      ],
      "metadata": {
        "id": "R976BTHqth_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "PjKikpD7uCnb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we wil be performing a simple Exploratory Data Anaysis for this project. We will use the methods we learned in the tutorials to have a basic understanding of the dataset. So first we will start with the heart dataset available from kaggle. the infomration about the columns of the dataset is given below:    \n",
        "-age    \n",
        "-sex    \n",
        "-chest pain type (4 values)    \n",
        "-resting blood pressure    \n",
        "-serum cholestoral in mg/dl    \n",
        "-fasting blood sugar > 120 mg/dl    \n",
        "-resting electrocardiographic results (values 0,1,2)    \n",
        "-maximum heart rate achieved    \n",
        "-exercise induced angina   \n",
        "-oldpeak = ST depression induced by exercise relative to rest    \n",
        "-the slope of the peak exercise ST segment    \n",
        "-number of major vessels (0-3) colored by flourosopy    \n",
        "-:thal: 0 = normal; 1 = fixed defect; 2 = reversable defect    "
      ],
      "metadata": {
        "id": "Ep9v4Rj1rDaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the portions that says \"to do\""
      ],
      "metadata": {
        "id": "GVPNEAfBrs7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded1 = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "I9sX_JSdtpsH",
        "outputId": "ce0ba944-aba7-483b-eb96-9a042c6f9d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7ae2ff5f-2b49-44cb-842c-2bf2497b75af\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7ae2ff5f-2b49-44cb-842c-2bf2497b75af\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"heart.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "7CUciJW-t-aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "j3fHpCWfuWSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns\n"
      ],
      "metadata": {
        "id": "DuxMngbivcdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "N1OCngCVr5H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## we will be comparing rest of the parameters/columns present in the data with respect to precence or absece of heart disease\n",
        "data['target'] = data.target.replace({1: \"Disease\", 0: \"No_disease\"})\n",
        "data['sex'] = data.sex.replace({1: \"Male\", 0: \"Female\"})\n",
        "data['cp'] = data.cp.replace({1: \"typical_angina\",\n",
        "                          2: \"atypical_angina\",\n",
        "                          3:\"non-anginal pain\",\n",
        "                          4: \"asymtomatic\"})\n",
        "data['exang'] = data.exang.replace({1: \"Yes\", 0: \"No\"})\n",
        "data['slope'] = data.cp.replace({1: \"upsloping\",\n",
        "                          2: \"flat\",\n",
        "                          3:\"downsloping\"})\n",
        "data['thal'] = data.thal.replace({1: \"fixed_defect\", 2: \"reversable_defect\", 3:\"normal\"})"
      ],
      "metadata": {
        "id": "fRHrjko7xIwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "wl5oM9BZ2XVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets look at the difference in the number of samples with and without disease using a barplot."
      ],
      "metadata": {
        "id": "RKjU-NUW3op-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data['target'].value_counts())\n",
        "plt.title('Heart Disease Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9H3aQvCe2prB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we can plot the same barplots usng the pandas inbuilt plotting functions.\n",
        "data['target'].value_counts().plot(kind='bar').set_title('Heart Disease Classes')"
      ],
      "metadata": {
        "id": "UpL7yRHR2ZA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now plot a barplot indicating the the sex of the participants involved in the study, use whatever method of ploting comfortable for you\n",
        "## to do"
      ],
      "metadata": {
        "id": "J3pWoE-j282Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pie charts can also be used to show the same infomation in a different manner\n",
        "plt.pie(data['target'].value_counts(), labels=[\"Disease\", \"No disease\"], autopct='%1.1f%%')\n",
        "plt.title('Target Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6lzeMeXx6Ddl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next we will plot the counts of all the non-continous features present in the dataset.\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    sns.barplot(data[feature].value_counts(), ax=ax)"
      ],
      "metadata": {
        "id": "lhrwKDM62l67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  now lets play with 2 vaiables in dataset. Lets see if chest pain translates to the presence of desease in most cases...\n",
        "sns.countplot(x='cp', hue='target', data=data, palette='rainbow').set_title('Disease classes according to Chest Pain')"
      ],
      "metadata": {
        "id": "5MNCxVhQ4x30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets visualise count of all vairables w.r.t the presence of disease togather:\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    ## to do\n"
      ],
      "metadata": {
        "id": "smGhn01t5bqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the distribution of the continous variables"
      ],
      "metadata": {
        "id": "V0CiDMIK6pCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pair plots can automoaticaly be used to viwe the pairwise relationship between all the  feature that we selected\n",
        "continous_features = ['age', 'chol', 'thalach', 'oldpeak','trestbps']\n",
        "sns.pairplot(data[continous_features + ['target']], hue='target')"
      ],
      "metadata": {
        "id": "6KnGCZz632hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets try to understand the relationship between age and chol in each of the target based on sex.\n",
        "sns.lmplot(x=\"age\", y=\"chol\", hue=\"sex\", col=\"target\",\n",
        "           palette=\"Set1\",\n",
        "           data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DtjSCsgl6vV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = data[continous_features]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numeric_data.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ywPbmTHx8Rqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, len(continous_features), figsize=(25, 4), sharex=False, sharey=False)\n",
        "\n",
        "for idx, feature in enumerate(continous_features):\n",
        "    sns.boxplot(x='target', y=feature, data=data, ax=axes[idx])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOQvj2UT-4Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot the cumulative variace of pca for all the possibel pronviopal components\n",
        "## to do\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca = PCA()\n",
        "pca.fit(numeric_data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QLy3U9yOAAa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(numeric_data)\n",
        "pca_data = pca.transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the principal components and target labels\n",
        "pca_df = pd.DataFrame({\n",
        "    \"pca_1\": pca_data[:, 0],\n",
        "    \"pca_2\": pca_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the PCA results with a scatter plot\n",
        "sns.scatterplot(x=\"pca_1\", y=\"pca_2\", hue=\"target\", data=pca_df)\n",
        "plt.title(\"PCA Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9kb_TztC9hqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Initialize and fit the TSNE model\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_data = tsne.fit_transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the TSNE components and target labels\n",
        "tsne_df = pd.DataFrame({\n",
        "    \"tsne_1\": tsne_data[:, 0],\n",
        "    \"tsne_2\": tsne_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the TSNE results with a scatter plot\n",
        "sns.scatterplot(x=\"tsne_1\", y=\"tsne_2\", hue=\"target\", data=tsne_df)\n",
        "plt.title(\"TSNE Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QDKt3GQIAMHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Percentage of Samples with Disease**\n",
        "\n",
        "The proportion of samples with disease can be obtained in a similar manner by dividing the number of people with the disease by the total number of samples and then multiplying by 100. For instance, if there are 35 people out of 70 people in a sample, then have a specific disease, then the percentage of samples with disease would be 50 %.\n",
        "\n",
        "However, the proportion of samples that have disease in a population can be quite different depending upon the proportion of diseased people in that population. For example, if the overall rate of disease in the population is 10%, the proportion of samples with disease will also be 10%.\n",
        "\n",
        "To determine the sample size required to estimate the prevalence of a disease, you can use the following formula:\n",
        "\n",
        "Sample size = (Z^2 * σ^2) / E^2\n",
        "\n",
        "Where:\n",
        "\n",
        "Z is the Z-score that would correspond to the specific level of confidence that is needed, for instance a 95% level of confidence will have its corresponding Z-score of 1.96\n",
        "Σ is the standard deviation of the population.\n",
        "E is number of times the sample is taken to ensure it is within the acceptable range of mean.\n",
        "For example, if you want to estimate the prevalence of a disease with a 95% confidence level and a margin of error of 5%, and the standard deviation of the population is 0.1, the sample size would be:\n",
        "\n",
        "Sample size = (1.96 * 1.96 * 0.1 * 0.1) / (0.05 * 0.05) = 384.16\n",
        "\n",
        "Thus, to achieve the required level of accuracy in identifying the prevalence of the disease, you would need a sample of at least 385 people.\n",
        "\n",
        "What one should remember, however, is that the sample size that is needed can also depend on the frequency of the disease occurrence within the population and the margin of error."
      ],
      "metadata": {
        "id": "7R5Vn92ZhpS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **#The 3 continous features that shows a singnficanct statistical differnce in distribution with respect to the precence and absence of the disease**\n",
        "\n",
        "\n",
        "If we wish to identify which of the three continuous variables have different distributions where there is the disease and where there is not, one can use a two sample test such as the t-test or Wilcoxon rank-sum test.\n",
        "\n",
        "Supposing the data encompasses n samples, each of which consists of m contineous variables and one binary variable, that indicates the presence or absence of the disease.\n",
        "\n",
        "Here are the steps to identify the three continuous features that show a significant statistical difference:\n",
        "\n",
        "Split the data: Use a binary classification where you allocate data into classes that contain class 1 and classes that contain class 0 if the data contains the disease.\n",
        "Perform two-sample tests: To compare the two groups of data, it is possible to reject the null hypothesis of equal distribution of the m continuous features between the two groups using two sample test such as the t-test test on the feature set for every feature or the Wilcoxon rank sum test.\n",
        "Calculate p-values: Use an independent or t-test to compute the p-value for each of the features which tell you the significance level of the observed difference in the distribution of the feature under consideration between two groups in question that can be attributed to chance alone.\n",
        "Rank features by p-value: Sort the features in the order where the p-values will help to sort, that means sort them in increasing order of p-values.\n",
        "Select the top 3 features: Select the three features having the smallest p-values because these are always the measures that exhibit maximum divergence of the distributions of the populations."
      ],
      "metadata": {
        "id": "GlVxbBQ9kGZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "# Split the data into two groups based on the presence/absence of the disease\n",
        "group0 = df[df['disease'] == 0]\n",
        "group1 = df[df['disease'] == 1]\n",
        "\n",
        "# Perform two-sample tests on each feature\n",
        "p_values = []\n",
        "for feature in df.columns[:-1]:  # exclude the disease label\n",
        "    t_stat, p_val = ttest_ind(group0[feature], group1[feature])\n",
        "    p_values.append((feature, p_val))\n",
        "\n",
        "# Rank features by p-value\n",
        "p_values.sort(key=lambda x: x[1])\n",
        "\n",
        "# Select the top 3 features\n",
        "top_features = [x[0] for x in p_values[:3]]\n",
        "\n",
        "print(\"Top 3 features with significant statistical differences:\")\n",
        "print(top_features)"
      ],
      "metadata": {
        "id": "Qt2JzOqwkgsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#We see a clear seperation in terms of the presence/absence of disease in the features obtained from pca and tsne plots**\n",
        "\n",
        "To visualize the separation of the presence/absence of disease in the features obtained from PCA and t-SNE plots, we can create scatter plots of the first two principal components (PCs) from PCA and the first two dimensions from t-SNE."
      ],
      "metadata": {
        "id": "WHD1vwF5l2B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(df.drop('disease', axis=1))\n",
        "\n",
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_features = tsne.fit_transform(df.drop('disease', axis=1))\n",
        "\n",
        "# Create scatter plots\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "ax[0].scatter(pca_features[:, 0], pca_features[:, 1], c=df['disease'])\n",
        "ax[0].set_title('PCA')\n",
        "ax[0].set_xlabel('PC1')\n",
        "ax[0].set_ylabel('PC2')\n",
        "\n",
        "ax[1].scatter(tsne_features[:, 0], tsne_features[:, 1], c=df['disease'])\n",
        "ax[1].set_title('t-SNE')\n",
        "ax[1].set_xlabel('Dimension 1')\n",
        "ax[1].set_ylabel('Dimension 2')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cPrE-SLImBYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we explore how to perform PCA as well as t-SNE on the dataset with the dimensionality brought down to a mere two. We then plot the PCA using the first two Principal Components and t-SNE using the first two dimensions of the data and color the points based on the presence or absence of the disease coded as 1 and 0 respectively.\n",
        "\n",
        "That is, if, perhaps, PCA, or t-SNE of the features reveals that there is a probability that there exists a significant variance between a group of people diagnosed with the disease and another group of people without the disease diagnosis, directions of the scatter plot of the data set should be distinguishable. For instance, if the disease has positive correlation with both, PC1 & PC2; points with disease=1 will likely be in the top-right corner of the PCA plot.\n",
        "\n",
        "However, if these two categories of features are not defined well, it perhaps may suggest that these features are not closely related with the disease presence or absence levels or if PCA and tSNE are applied to reduce the number of features.\n",
        "\n",
        "Here are some possible outcomes:\n",
        "\n",
        "Clear separation: Our PCA results show quite different clusters or patterns in the scatter plots, which mean that the features returned from the two tools of PCA and t-SNE are highly correlated with the existence of disease.\n",
        "Partial separation: Some of the clusters are somewhat distinct between presence and absence of disease, but the clusters are also somewhat smeared.\n",
        "No separation: As such, it is possible to conclude that case and non-case groups are equally distributed and the PCA and t-SNE features are independent of the disease.\n",
        "When visualizing the separation of presence and absence of disease in the features derived from PCA and t-SNE, important features of the structure of the data and the performance of the dimensionality reduction can be observed."
      ],
      "metadata": {
        "id": "BU_nYx2ZmbCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#The optimal number of principal components in our case**\n",
        "\n",
        "When deciding where the number of principal components in question is going to be, it is possible to assess the variance that has been explained by the choice of the number of principal components in each case.\n",
        "\n",
        "Here are some common methods to determine the optimal number of PCs:\n",
        "\n",
        "**Scree plot:** In order to illustrate what a scree plot is, it is just sort of a graph of the eigenvalues of the PCs sorted in ascending or descending order. The locations of the eigenvalues can also be used to pin point the elbow point, which is when the values start to plateau, meaning that the PCs that are remaining only explain very little variance.\n",
        "Cumulative explained variance: This will afford us the opportunity to compute the cumulative explained variance for each PC and retain only that number of PCs, which would yield the given percentage of total variance say 80% or 90%.\n",
        "\n",
        "**Kaiser's criterion:** This method suggests that all the PCs with eigenvalue > 1 should remain seized because they explained more variance than one of the original feature.\n",
        "\n",
        "**Cross-validation:** We can cross-validate the performance of the model with various numbers of PCs and decide with number of PCs gives the best performance.\n",
        "Now, let me continue with the scree plot and cumulative explained variance in an effort to determine the right number of the particular PCAs.\n",
        "\n",
        "Here's an example of how you could create a scree plot and calculate the cumulative explained variance in Python using the matplotlib and sklearn libraries:\n"
      ],
      "metadata": {
        "id": "ySL2tAlBmvNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "pca_features = pca.fit_transform(df.drop('disease', axis=1))\n",
        "\n",
        "# Create scree plot\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.show()\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cum_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "print(\"Cumulative Explained Variance:\")\n",
        "print(cum_explained_variance)\n",
        "\n",
        "# Select the number of PCs that explain 80% of the variance\n",
        "n_components = np.argmax(cum_explained_variance >= 0.8) + 1\n",
        "print(\"Optimal number of PCs:\", n_components)"
      ],
      "metadata": {
        "id": "voVf8Iyhm9D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Identifying Continuous Features with the Highest Correlation**\n",
        "\n",
        "First, if a method is to be generated that could identify the constant variables that are most correlated to each other then the correlation coefficients of all constant variables in the data set may be computed.\n",
        "\n",
        "Here's an example of how you could calculate the correlation coefficients in Python using the pandas library:"
      ],
      "metadata": {
        "id": "bgW-KFeSnI5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "# Select only continuous features\n",
        "continuous_features = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Calculate pairwise correlation coefficients\n",
        "corr_matrix = df[continuous_features].corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(\"Correlation Matrix:\")\n",
        "print(corr_matrix)"
      ],
      "metadata": {
        "id": "SLMyBgdtnzEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we will only choose the continuous variables in the data frame and then use the corr() function to determine the correlation coefficients between each two variables. This makes it possible for the correlation matrix to show, in correlation coefficients how each of the continuous features can be correlated with another feature.\n",
        "\n",
        "Therefore one might sort the correlation matrix according to the absolute correlation coefficients in order to diagnose the features with the highest correlations. Here's an example:"
      ],
      "metadata": {
        "id": "8jHEQ4p-oFt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the correlation matrix by absolute value of correlation coefficients\n",
        "sorted_corr_matrix = corr_matrix.abs().sort_values(ascending=False)\n",
        "\n",
        "# Print the top 5 correlated feature pairs\n",
        "print(\"Top 5 Correlated Feature Pairs:\")\n",
        "print(sorted_corr_matrix.head(5))"
      ],
      "metadata": {
        "id": "bsEHNbXjoJEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of this example, we rank the correlation matrix by the absolute value of the correlation coefficients and only print the five most correlated features.\n",
        "\n",
        "From the analysis of the correlation matrix, it is possible to spot some of the continuous features with high levels of correlation. For instance, the conclusion that when coefficient of determination is greater than equal to zero point eight then it will point or lead to infer linearity between two features.\n",
        "\n",
        "Here are some possible outcomes:\n",
        "\n",
        "**High correlation:**\n",
        "This is done by identifying those feature pairs with high ρ values representing the extent of an association between two features, which can be defined by a linear function.\n",
        "Moderate correlation: Finally, the moderate values of correlation coefficients implies that there exists weaker linearity in features.\n",
        "\n",
        "**Low correlation:** Pertaining to continuous features being independent, we do not find any feature pairs that have high correlation coefficients.\n",
        "Further, since continuous variables represent dimensions in the dataset the study of the covariances across these associations give insights into the organization of the data and the relations of the features."
      ],
      "metadata": {
        "id": "2eyg-8ZtofUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets move on to do the same analysis on the starbucks nutrition dataset. this dataset contains the nutrition information of starbucks drinks."
      ],
      "metadata": {
        "id": "w8hH8CAhv8tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upload2 = files.upload()"
      ],
      "metadata": {
        "id": "a_RrGpN1ApCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"star_nutri_expanded.csv\")"
      ],
      "metadata": {
        "id": "NImD-yk0fpmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "YXZJPk4Zf636"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning and filling the missing values in the data"
      ],
      "metadata": {
        "id": "2JA9ZVyywXk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].replace('Varies', np.NaN).replace('varies', np.NaN)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].astype(np.float64)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].fillna(data['Caffeine (mg)'].mean())"
      ],
      "metadata": {
        "id": "VrE3QA0GgCTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'].unique()"
      ],
      "metadata": {
        "id": "0PNMhN2GiQ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'] = data['Total Fat (g)'].replace('3 2', '3.2')"
      ],
      "metadata": {
        "id": "Suua5EkTiRxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "u-I6TsTLkiMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract columns with int and float types\n",
        "numeric_columns = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Print the numeric columns\n",
        "print(numeric_columns)\n"
      ],
      "metadata": {
        "id": "xPfMl26LknBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be analysing the dataset using the fact that weather the drink comes under the category tea or not"
      ],
      "metadata": {
        "id": "9FY9pzBNwete"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Beverage_category'].unique()"
      ],
      "metadata": {
        "id": "LVljNoJgg9Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Tea'] = data['Beverage_category'].apply(lambda x: 1 if x == 'Tazo® Tea Drinks' else 0)\n",
        "data = data.drop('Beverage_category', axis=1)"
      ],
      "metadata": {
        "id": "4UZ4Tk7whVEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  one hot encoding of categorical features in data\n",
        "def onehot_encode(df, columns, prefixes):\n",
        "    df = df.copy()\n",
        "    for column, prefix in zip(columns, prefixes):\n",
        "        dummies = pd.get_dummies(df[column], prefix=prefix)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df = df.drop(column, axis=1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "OZanK3hSi6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = onehot_encode(\n",
        "    data,\n",
        "    columns=['Beverage', 'Beverage_prep'],\n",
        "    prefixes=['bev', 'bevp']\n",
        ")"
      ],
      "metadata": {
        "id": "PlZCj2rwi7us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = data.replace({True: 1, False: 0})\n"
      ],
      "metadata": {
        "id": "w7qKp8JMwysG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.applymap(lambda x: np.float64(str(x).replace('%', '')))"
      ],
      "metadata": {
        "id": "I-RIoXq6ig3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "J_BgqsKogWE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "iB5PmiPojj3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "\n",
        "# Create a pie chart of the 'Tea' column also write your observation form the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "KxZYShn1hpYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# perform pca on the data and plot the explained variace ratio, what is the optimal number of principal components in this case ?\n"
      ],
      "metadata": {
        "id": "HL2dDjYfh2L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# visualise the principal components, choose the number of principal components based on the above plot. What is you observation from the plot?\n"
      ],
      "metadata": {
        "id": "tNKX2JJVjwrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# plot the first 2 components of tsne, whats you observation from the plot?\n"
      ],
      "metadata": {
        "id": "tgpIgiYdlVcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# create a correlation matrix and plot the heatmap, whats your observation from the heatmap ?\n"
      ],
      "metadata": {
        "id": "cOWaPpnmj4OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# make a boxplot of all the numeric columns of the dataset. Which column/columns can be the most potential indicator weather its a tea or a non tea drink?\n"
      ],
      "metadata": {
        "id": "VJMRR3yqkNb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the clarity and professionalism of the provided text, consider the following refined version: In the process of conducting a preliminary Exploratory Data Analysis (EDA), we have utilized various techniques to gain insights into the datasets under consideration. It's important to note that our analysis extends beyond the initial visualizations, embracing a multitude of methods to thoroughly understand the data.\n",
        "Among the array of tools available for EDA, one particularly easy solution is the use of the pandas profiling library. This tool significantly simplifies the process of exploring the fundamental distribution of data within a dataset. By generating detailed profile reports, pandas profiling provides a comprehensive overview of the dataset's characteristics, including but not limited to, the distribution of variables, presence of missing values, and potential correlations between variables.\n",
        "Furthermore, we are utilizing Google Colab notebooks, the integration of AI tools offers an additional avenue for data visualization and analysis. These tools can automatically generate insightful plots and statistics, further enriching the data exploration process."
      ],
      "metadata": {
        "id": "_yRYeXpfzY7O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDevVx8Bo8lN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}